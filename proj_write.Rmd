---
title: "cross institution draft"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F, echo = F, warning = F}
library(ProbBayes)
library(tidyverse)
library(bayesrules)
library(patchwork)
library(ggplot2)
library(runjags)
library(bayesplot) 
```



```{r, echo=FALSE, out.width='50%'}
between<- read.csv("proj_11.14.csv")
#View(between)
cross<- between %>% mutate(Test_Rate = Total_Tests/Enrollment)
cross<- cross %>% mutate(type_num = ifelse(Type  == "LAC", 1, 0))


modelString_mu <-"
model {
## likelihood
for (i in 1:N){
   y[i] ~ dbin(p[i], n[i])
}

## priors and regression
for (i in 1:N){
   p[i] ~ dbeta(a[i], b[i])
   a[i] <- phi[i] * mu[i]
   b[i] <- phi[i] * (1 - mu[i])
   logit(mu[i]) <- beta0 +beta1*type[i] +beta3*test_rate[i]
   phi[i] <- exp(logeta[i])
   logeta[i] ~ dlogis(logn, 1)
   
}
## hyperpriors

beta0 ~ dnorm(0, 0.01)
beta1 ~ dnorm(0, 0.001)
beta3 ~ dnorm(0, 0.001)

## predictive 
for (i in 1:N) {
   y_pred[i] ~ dbin(p[i], n[i])
}

}

"
init = list(
            list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 987654), 
            list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 987653),
            list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 987652)
            )




y <- cross$Positive_Results
n <- cross$Total_Tests
type <- cross$type_num
#pop <- cross$Enrollment
test_rate<- cross$Test_Rate
N <- length(y)
the_data2 <- list("y" = y, "n" = n, "N" = N, "type" = type, "test_rate" = test_rate,
                 "logn" = log(100))



seed = list(
            list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 987654), 
            list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 987653),
            list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 987652)
            )


posterior_mu <- run.jags(modelString_mu,
                 data = the_data2,
                 n.chains = 3,
                 monitor = c("beta0","beta1","beta3", "p", "y_pred"),
                 adapt = 1000,
                 burnin = 5000,
                 sample = 10000,
                 thin = 5,
                 inits = init, silent.jags = TRUE)
```


## Methods

We decide to use Beta-Binomial models to study the positive COVID test rates across these institutions. In our setting, we assume that the number of people tested as positive follows a binomial distribution with each test being an independent Bernoulli trial. We also think that there exist potential factors that could affect the positive test rate of these institutions and the relationship between the positive test rate and these factors are linear. Some other assumptions we have are independent and constant variance. The factors considered include the institution type (University versus LAC), institution enrollment, and the test rate (calculated as the total number of tests administrated from 2020 to 2021 divided by enrollment). We also assume that there exists some baseline positive test rate for all educational institutions in the US. These factors will be included in our model as regressors in the link function of positive test rate $p_{j}$, where the subscript j denotes the specific institution.

For the first model (Model1), we used the logistic regression as a link function on $p_{j}$, and the regressors included are the intercept, the institution type, the standardized enrollment, and the standardized test rate. We assume that the intercept $\beta_{0}$, the coefficient for institution type $\beta_{1}$, the coefficient for standardized enrollment $\beta_{2}$, and the coefficient for standardized test rate $\beta_{3}$ all follows weakly informative priors $N(0, 100)$. Below are the equations for Model1: 

+ Sampling: for j in 1,...30:
\begin{equation}
\begin{gathered}
\mathbf{Y}_{j}|\mathbf{p}_{j}, \mathbf{n}_{j} \stackrel{i.i.d.}{\sim} \binom{\mathbf{n}_{j}}{\mathbf{p}_{j}}\\
logit(\mathbf{p}_{j}) = \mathbf{\beta_{0}} +  \mathbf{\beta_{1}} * Type + \mathbf{\beta_{2}} * Zscore\hspace{0.1cm}Enrollment + \mathbf{\beta_{3}} * Zscore\hspace{0.1cm}TestRate
\end{gathered}
\end{equation}



+ Prior for $\mathbf{p}_{j}$, for j= 1,...30:
\begin{equation}
\begin{gathered}
\mathbf{\beta_{0}} \sim \mathcal{N}(0,\,100)\\
\mathbf{\beta_{1}} \sim \mathcal{N}(0,\,100)\\
\mathbf{\beta_{2}} \sim \mathcal{N}(0,\,100)\\
\mathbf{\beta_{3}} \sim \mathcal{N}(0,\,100)
\end{gathered}
\end{equation}


In the second model (Model2), we assume that our positive test rate $p_{j}$ follows a Beta distribution with parameters $a_{j}$ and $b_{j}$. We will use reverse elicitation to make posterior inferences on $a_{j}$ and $b_{j}$. We define the mean of the Beta distribution as $\mu_{j} = \frac{a_{j}}{a_{j} +b_{j}}$ and the sample size as $\eta = a_{j} +b_{j}$. Then we use the logistic regression as a link function on $\mu_{j}$ with regressors as the intercept, the institution type, and the test rate. Similarly, we assume the intercept $\beta_{0}$, the coefficient for institution type $\beta_{1}$, and the coefficient for test rate $\beta_{3}$ all follows weakly informative priors $N(0, 100)$. The equations for Model2 are shown below:

+ Sampling: for j in 1,...30:
\begin{equation}
\begin{gathered}
\mathbf{Y}_{j}|\mathbf{p}_{j}, \mathbf{n}_{j} \stackrel{i.i.d.}{\sim} \binom{\mathbf{n}_{j}}{\mathbf{p}_{j}}\\
\mathbf{p}_{j} \sim {Beta}(a_{j}, b_{j})
\end{gathered}
\end{equation}



+ Prior for $\mathbf{p}_{j}$, for j= 1,...30:
\begin{equation}
\begin{gathered}
\mathbf{a_{j}} = \phi_{j} * \mu_{j}\\ 
\mathbf{b_{j}} = \phi_{j} * (1 - \mu_{j})\\
logit(\mu_{j}) = \beta_{0} +\mathbf{\beta_{1}} * type_{j} + \mathbf{\beta_{3}} * TestRate_{j}\\
\phi_{j} = exp(logeta_{j})\\
logeta_{j} \sim Logistic(logn, 1)
\end{gathered}
\end{equation}



+ Hyperprior
\begin{equation}
\begin{gathered}
\mathbf{\beta_{0}} \sim \mathcal{N}(0,\,100)\\
\mathbf{\beta_{1}} \sim \mathcal{N}(0,\,100)\\
\mathbf{\beta_{3}} \sim \mathcal{N}(0,\,100)\\
logn = log(100)
\end{gathered}
\end{equation}

For both Model1 and Model2, we made an additional 10000 draws after an adaptation period of 1000 draws and a burn-in period of 5000 draws, and we kept every 5th draws to reduce the effect of temporal correlation between consecutive MCMC draws. For convergence consistency, we ran 3 chains for both models.


• \ Diagnostics: 
For Model1, the trace plots and ACF plots show that our draws are well-mixed and our parameters have converged to their posterior region. The overlaid density plots show that the draws from 3 separate chains all converged to the same distribution with roughly the same density curves. To make sure that Model1 gives realistic predictions as what we would see in real life, we simulated data sets using Model1 and compared them to the observed data. We first checked the overlayed density plot; of the 100 simulated data sets we randomly selected, they all have similar density curves as the density curve of the observed data. Then we compared the observed mean number of the positive test of each institution to the distribution of the 10000 mean number of the positive test of the simulated data to check if the posterior estimates of the mean of number the positive test using $p_{j}$ are reasonable. We discovered that the observed mean lies in the center of the distribution of the simulated data mean for almost all institutions, which indicates that a data set like the one we observed is not uncommon compared to the data simulated using Model1, which confirmed that Model1 is adequate. 

For Model2, we also see that our draws are well-mixed and our parameters have converged to their posterior region from checking the trace plots and ACF plots. The overlaid density plots also show that the draws from three separate chains all converged to the same distribution with roughly the same density curves. Similarly, we also conducted the posterior predictive check on Model2. The 100 simulated data have a very similar density curve to the density curve of the observed data. As in Model1, checking the posterior estimates of the mean number of positive tests confirmed the fact that our model is able to generate data that mimic the observed data, and therefore we will be able to conclude that Model2 is adequate as well. 

## Results

With model1, we know that if the enrollment number and test rate are at their average level among these institutions, the positive test rate of a University is 0.0142 ($\beta_{0}$), and for a LAC is 0.0071 ($\beta_{0} +\beta_{1}$. Looking at the credible intervals for these coefficients, we also know there is a 95% chance that the baseline positive test rate $\beta_{0}$ for University is between 0.0140 to 0.0143; for $\beta_{1}$, we know that the baseline positive test rate of LAC is from # to # lower than the baseline positive test rate of University. 
 

Holding the institution type and test rate constant, with every one standard deviation increase in the enrollment, there will be a 132% increase in the probability of being tested positive ($\beta_{2}$), and we know that this decrease will range from 130% to 134% with a 95% probability. Holding the institution type and enrollment constant, with every one standard deviation increase in the test rate, there will be a 44.59% decrease in the probability of being tested positive ($\beta_{3}$), and we know that this increase will be from 43.87% to 45.29% with a 95% probability.


The posterior inference made on $p_{j}$ using Model1 is shown in Fig.# below:
```{r, echo = F, fig.align="center"}
post_intervals <- mcmc_intervals_data(posterior_mu$mcmc[, 4:33], regex_pars = "p", prob_outer = 0.9) %>% mutate(para = c("Smiths College","Amherst College","Middlebury College","Williams College", "Colby College", "Stanford University", "Harvard University", "Boston University", "Johns Hopkins University", "Bowdoin College", "University of Chicago", "Rice University", "Carleton College", "University of Illinois, Urbana-Champaign", "Macalester College", "Bryn Mawr College", "University of California Berkely", "St. Olaf College", "University of Pennsylvania", "Washington and Lee University", "Ohio State University", "University of Arizona", "University of Washington", "University of Miami", "Pennsylvania State University", "University of Texas Austin", "Purdue University", "University of California Los Angeles", "Univsersity of Michigan", "University of Minnesota Twin City"))

slice(post_intervals, 1:30) %>%
  ggplot(
    aes(x = reorder(para, (m)), y = (m), ymin = (ll), ymax = (hh))) +
  geom_pointrange() +
  theme_light() +
  xaxis_text(angle = 0, size = 6) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
)+ xlab("School") + ylab("Positive Test Rate") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


Using Model2, we know that if the test rate is at their average level among these institutions, the mean of the distribution of positive test rate of a University is 0.0452 ($\beta_{0}$), and for LAC is 0.0152 ($\beta_{0} +\beta_{1}$. There’s a 95% chance that the baseline positive test rate for University is from # to #, and for LAC is from # to #. Holding the institution type constant, with every one standard deviation increase in the test rate, there will be a 95.2% increase in the positive test rate ($\beta_{3}$). This increase could range from # to #, with a 95% possibility. 


